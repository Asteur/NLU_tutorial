{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Concept Extraction and Word Embeddings</h4>\n",
    "\n",
    "Problem:  \n",
    "\n",
    "   Find relationships between Disease or Syndromes, Signs or Symptoms and Chemicals and Drugs and Anatomy parts in Pubmed Articles on Type I Diabetes.\n",
    "\n",
    "Solution:\n",
    "   \n",
    "   Using [BioPython] (http://biopython.org/DIST/docs/tutorial/Tutorial.html) we're searching Pubmed for articles on Type I Diabetes. \n",
    "\n",
    "   We're extracting medical terms using UMLS (an ontlogy) utilizing QuickUMLS (a Spacy based library) and we filter the content of the articles on just these two semantic types.\n",
    "   \n",
    "   We're training Word2Vec on these filtered articles and we analyze the relationships computed based on these embedding vectors.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "search_term = 'type+1+diabetes[MH]'\n",
    "max_articles = 100\n",
    "\n",
    "from Bio import Entrez\n",
    "print('Searching PubMed abstracts for documents containing term: ',search_term)\n",
    "handle = Entrez.esearch(db=\"pubmed\", term=search_term, retmax=max_articles)\n",
    "record = Entrez.read(handle)\n",
    "handle.close()\n",
    "idlist = record[\"IdList\"]\n",
    "\n",
    "print('Found:',len(idlist),' documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fetching the previously found documents\n",
    "from Bio import Medline\n",
    "handle = Entrez.efetch(db=\"pubmed\", id=idlist, rettype=\"medline\",retmode=\"text\")\n",
    "records = Medline.parse(handle)\n",
    "data = []\n",
    "for record in records:\n",
    "    res = (record.get(\"TI\", \"?\"),record.get(\"AU\", \"?\"),record.get(\"SO\", \"?\"),record.get(\"AB\",\"?\"))\n",
    "    if res[3]!='?':\n",
    "        data.append(res)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data=data, columns=['Title','Authors','Source','Abstract'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using https://github.com/Georgetown-IR-Lab/QuickUMLS extract concepts out of these articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('QuickUMLS')\n",
    "from quickumls import QuickUMLS\n",
    "tagger = QuickUMLS('QuickUMLS/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(tagger)\n",
    "print(tagger.nlp)\n",
    "for obj in tagger.nlp.pipeline:\n",
    "    print('\\t',obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Load the complete list of Semantic Types available in UMLS\n",
    "sems = {}\n",
    "data=[]\n",
    "with open('SemGroups_2013.txt','r') as f:\n",
    "    for line in f:\n",
    "        lines = line.replace('\\n','').split('|')\n",
    "        cat = lines[1]\n",
    "        semtype = lines[2]\n",
    "        desc = lines[3]\n",
    "        sems[semtype]=(cat,desc)\n",
    "        data.append([semtype,cat,desc])\n",
    "import pandas as pd\n",
    "sdf = pd.DataFrame(data=data,columns=['SemType','Category','Tag'])\n",
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Extract medical concepts out of the first article\n",
    "txt = str('\\n\\n'+df['Abstract'].values[0])\n",
    "# print(txt)\n",
    "tagger.window = 5 #window used by the tagger to construct n grams from tokens\n",
    "matches= tagger.match(txt, best_match=True, ignore_syntax=False)\n",
    "\n",
    "stpwds = set()\n",
    "doc = tagger.nlp(txt)\n",
    "for token in doc:\n",
    "    if token.is_stop:\n",
    "        stpwds.add(token.idx)\n",
    "        \n",
    "    \n",
    "import pandas as pd\n",
    "tagged = {}\n",
    "data = []\n",
    "for match in matches:\n",
    "#     print match\n",
    "    semtypes = set()\n",
    "    term = ''\n",
    "    cui = ''\n",
    "    ngram = ''\n",
    "    mi=0\n",
    "    for m in match:\n",
    "        for s in m['semtypes']:\n",
    "            semtypes.add(s)\n",
    "        if m['similarity']>mi:\n",
    "            term = m['term']\n",
    "            cui = m['cui']\n",
    "            mi=m['similarity']\n",
    "            ngram = m['ngram']\n",
    "    if len(term)<=2:\n",
    "        continue\n",
    "    if match[0]['start'] in stpwds:\n",
    "        continue\n",
    "    tmp=[]\n",
    "    tmp.append(match[0]['start'])\n",
    "    tmp.append(match[0]['end'])\n",
    "    tmp.append(term)\n",
    "    tmp.append(cui)\n",
    "    tmp.append(mi)\n",
    "    stypes = set()\n",
    "    for sem in semtypes:\n",
    "        stypes.add(sems[sem][1])\n",
    "    tmp.append(stypes)\n",
    "    tagged[term]=stypes\n",
    "    data.append(tmp)\n",
    "    \n",
    "df_matches = pd.DataFrame(data=data, columns =['start','end','term','cui','similarity','semtypes'])\n",
    "import helper_methods\n",
    "# reload(helper_methods)\n",
    "html = helper_methods.generate_html(tagged,sems)\n",
    "tmp = txt\n",
    "for tk in tagged:\n",
    "    for tag in tagged[tk]:\n",
    "        tmp = tmp.replace(tk,'<mark data-entity=\"'+tag.replace(' ','_').replace(',','_').replace('&','_and_')+'\">'+tk+'</mark>')\n",
    "html+=tmp\n",
    "html+='</div>'\n",
    "from IPython.display import display,HTML\n",
    "display(HTML(html))\n",
    "display(df_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this cell contains the code for a helper function that generates HTML code to highlight the \n",
    "# extracted words along with their semantic types in UMLS\n",
    "\n",
    "# %%writefile helper_methods.py\n",
    "# import random\n",
    "# def get_random_color(pastel_factor = 0.5):\n",
    "#     return [(x+pastel_factor)/(1.0+pastel_factor) for x in [random.uniform(0,1.0) for i in [1,2,3]]]\n",
    "\n",
    "# def color_distance(c1,c2):\n",
    "#     return sum([abs(x[0]-x[1]) for x in zip(c1,c2)])\n",
    "\n",
    "# def generate_new_color(existing_colors,pastel_factor = 0.5):\n",
    "#     max_distance = None\n",
    "#     best_color = None\n",
    "#     for i in range(0,100):\n",
    "#         color = get_random_color(pastel_factor = pastel_factor)\n",
    "#         if not existing_colors:\n",
    "#             return color\n",
    "#         best_distance = min([color_distance(color,c) for c in existing_colors])\n",
    "#         if not max_distance or best_distance > max_distance:\n",
    "#             max_distance = best_distance\n",
    "#             best_color = color\n",
    "#     return best_color\n",
    "# def generate_html(tagged,sems):\n",
    "#     html='''<style>\n",
    "#    [data-entity] {\n",
    "#     padding: 0.25em 0.35em;\n",
    "#      margin: 0px 0.25em;\n",
    "#      line-height: 1;\n",
    "#      display: inline-block;\n",
    "#      border-radius: 0.25em;\n",
    "#      border: 0px solid;\n",
    "#     }\n",
    "\n",
    "# [data-entity]::after {\n",
    "#     box-sizing: border-box;\n",
    "#     content: attr(data-entity);\n",
    "#     font-size: 0.9em;\n",
    "#     line-height: 1;\n",
    "#     padding: 0.35em;\n",
    "#     border-radius: 0.35em;\n",
    "#     text-transform: uppercase;\n",
    "#     display: inline-block;\n",
    "#     vertical-align: middle;\n",
    "#     margin: 0px 0px 0.1rem 0.5rem;\n",
    "#     }\n",
    "#     '''\n",
    "#     colors ={}\n",
    "\n",
    "#     cols = []\n",
    "#     for sem in sems:\n",
    "#         tag_label = sems[sem][1].replace(' ','_').replace(',','_').replace('&','_and_')\n",
    "#         colors[tag_label]=generate_new_color(cols,pastel_factor = 0.1)\n",
    "\n",
    "#     for lbls in tagged.values():\n",
    "#         for lbl in lbls:\n",
    "#     #         print lbl\n",
    "#             tag_label = lbl.replace(' ','_').replace(',','_').replace('&','_and_')\n",
    "#             html+='''[data-entity][data-entity=\"'''+tag_label+'''\"] {\n",
    "#                     background: rgba('''+str(int(colors[tag_label][0]*255))+','+str(int(colors[tag_label][1]*255))+','+str(int(colors[tag_label][2]*255))+''', 0.6);\n",
    "#                     border-color: rgb(166, 226, 45); \n",
    "#                     color:rgba(255,255,255,1.0);\n",
    "#                     }\\n'''\n",
    "\n",
    "\n",
    "#     #         html+='[data-entity][data_entity=\"'+tag_label+'''\"]::after {\n",
    "#     #             background: rgba(66, '''+str(random.randint(0,255))+''', 45, 0.9);\n",
    "#     #             }\\n'''\n",
    "\n",
    "#     html+='</style>'\n",
    "#     html+='<div class = \"mmm\">'\n",
    "#     return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_document(id,txt,tagger):\n",
    "    stpwds = set()\n",
    "    #run the document through the NLP pipeline\n",
    "    doc = tagger.nlp(txt)\n",
    "    \n",
    "    #create a list with the indeces of the stop words\n",
    "    for token in doc:\n",
    "        if token.is_stop:\n",
    "            stpwds.add(token.idx)\n",
    "    #run the UMLS tagger        \n",
    "    matches= tagger.match(txt, best_match=True, ignore_syntax=True)\n",
    "    data = []\n",
    "\n",
    "    for match in matches:\n",
    "        semtypes = set()\n",
    "        term = ''\n",
    "        cui = ''\n",
    "        ngram = ''\n",
    "        mi=0\n",
    "        #for every match collect all the semantic types\n",
    "        #keep only the term with the highest macthing score (similarity)\n",
    "    \n",
    "        for m in match:\n",
    "            for s in m['semtypes']:\n",
    "                semtypes.add(s)\n",
    "            if m['similarity']>mi:\n",
    "                term = m['term']\n",
    "                cui = m['cui']\n",
    "                mi=m['similarity']\n",
    "                ngram = m['ngram']\n",
    "                \n",
    "        #filter out terms shorter than 3 chars\n",
    "        if len(term)<=2:\n",
    "            continue\n",
    "        #filter out stop words\n",
    "        if match[0]['start'] in stpwds:\n",
    "            continue\n",
    "            \n",
    "        tmp=[]\n",
    "        tmp.append(id)\n",
    "        tmp.append(match[0]['start'])\n",
    "        tmp.append(match[0]['end'])\n",
    "        tmp.append(term.lower())\n",
    "        tmp.append(cui)\n",
    "        tmp.append(mi)\n",
    "        stypes = set()\n",
    "        for sem in semtypes:\n",
    "            stypes.add(sems[sem][1])\n",
    "        tmp.append(stypes)\n",
    "        data.append(tmp)\n",
    "    return data\n",
    "\n",
    "\n",
    "#configure the UMLS tagger to anly accept certian Semantic Types (per our problem)\n",
    "tagger.accepted_semtypes=set()\n",
    "tagger.accepted_semtypes.add('T047') #Disease or Syndrome\n",
    "tagger.accepted_semtypes.add('T184') # Sign or Symptom\n",
    "\n",
    "for sem in sems:\n",
    "    if sems[sem][0]=='Anatomy':\n",
    "        tagger.accepted_semtypes.add(sem)\n",
    "for sem in sems:\n",
    "    if sems[sem][0]=='Chemicals & Drugs':\n",
    "        tagger.accepted_semtypes.add(sem)\n",
    "\n",
    "#Iterate over every document and extract the concepts\n",
    "i=-1        \n",
    "result = []\n",
    "for idx,row in df.iterrows():\n",
    "    try:\n",
    "        i+=1\n",
    "        if row['Abstract'] is None:\n",
    "            continue\n",
    "        annotations = process_document(i,str(row['Abstract']),tagger)\n",
    "        result.extend(annotations)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "df_matches = pd.DataFrame(data=result, columns =['document','start','end','term','cui','similarity','semtypes'])\n",
    "df_matches.sort_values(by=['document','start'],inplace=True)\n",
    "df_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_matches.to_pickle('df_matches_pubmed_diabetes_100.data')\n",
    "# import pickle\n",
    "# with open('df_matches_pubmed_diabetes_100.data','r') as f:\n",
    "#     df_matches = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#construct documents using only the extracted concepts\n",
    "data = []\n",
    "for enc,items in df_matches.groupby(['document']):\n",
    "    data.append((enc,'|'.join(items['term'].values)))\n",
    "new_arts = pd.DataFrame(data = data,columns=['document','content'])\n",
    "new_arts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_frequent_terms=[]\n",
    "for term,items in df_matches.groupby(['term']):\n",
    "    most_frequent_terms.append((term,len(items)))\n",
    "most_frequent_terms.sort(key=lambda tup:tup[1],reverse=True)\n",
    "print('Most frequent terms: ',most_frequent_terms[:10])\n",
    "print('Vocabulary size: ',len(most_frequent_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We're using Spark's MLlib implementaiton of Word2Vec\n",
    "\n",
    "#initialize the Spark context\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import HiveContext, SQLContext\n",
    "sc = SparkContext('local','example_notebook')\n",
    "ssc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a SparkSql table containg the newly created documents\n",
    "df_new_arts = ssc.createDataFrame(new_arts[['document','content']])\n",
    "ssc.registerDataFrameAsTable(df_new_arts,tableName='new_arts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Train Word2Vec on this data \n",
    "new_notes = ssc.sql('select * from new_arts')\n",
    "from pyspark.mllib.feature import Word2Vec\n",
    "word2vec = Word2Vec()\n",
    "word2vec.setVectorSize(10) #embeddig vector size is 10\n",
    "model = word2vec.fit(new_notes.rdd.map(lambda x: x.content.split('|')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "\n",
    "def get_relevant_terms(term,cnt):\n",
    "    synonyms=[]\n",
    "    try:\n",
    "        synonyms = model.findSynonyms(term, cnt) #get the most relevant cnt terms by computing vector distance\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    return synonyms\n",
    "\n",
    "# construct the represeantion of the graph of concepts relationships derived from related terms obtained from word2vec\n",
    "# by selecting the top 5 most relevant concepts where the releavncy is >0.5 for the most frequent diseases and drugs concepts\n",
    "# having a frequency > 100 in the input 100 notes\n",
    "\n",
    "import networkx as nx\n",
    "G=nx.Graph()\n",
    "d_d=set()\n",
    "\n",
    "for termf in most_frequent_terms:\n",
    "    term = termf[0]\n",
    "    if ' ' not in term or termf[1]<2:\n",
    "        continue\n",
    "    rel = get_relevant_terms(term,3)\n",
    "    for tup in rel:\n",
    "        if tup[1]>0.1:\n",
    "            G.add_edge(term.replace(' ','_'),tup[0].replace(' ','_'),weight=round(tup[1],2))\n",
    "            d_d.add(term.replace(' ','_'))\n",
    "            \n",
    "d_d_l=list(d_d)\n",
    "try: \n",
    "    pos = nx.spring_layout(G,iterations=100)\n",
    "    plt.figure(figsize=(20,20))\n",
    "    edgewidth = [ d['weight'] for (u,v,d) in G.edges(data=True)]\n",
    "    nx.draw_networkx_nodes (G,pos,alpha=0.2,node_color='red',node_size=400)\n",
    "    nx.draw_networkx_nodes (G,pos,alpha=0.6,nodelist=d_d_l,node_color='yellow',node_size=500)\n",
    "    nx.draw_networkx_labels(G,pos,alpha=0.4,label_color='grey',font_size=10)\n",
    "    nx.draw_networkx_edges(G,pos,edge_color='orange',width = 1)\n",
    "    plt.savefig(\"disease_drugs.png\")\n",
    "    plt.show()\n",
    "except Exception as e: # matplotlib not available\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
