{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>How to build a document classifier ? </h4>\n",
    "Problem:\n",
    "\n",
    "We want to build a model that is able to tell if a Pubmed article is refering to child or adult patient(s).\n",
    "\n",
    "Solution:\n",
    "\n",
    "We construct a training/validation set out of English only Pubmed articles and use the keywords associated with these articles to assign the labels.\n",
    "\n",
    "Using Keras with Tensorflow backend, we train a convolutional neural network on the training data. We show very good accuracy and f1-score can be obtained in 5 Epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://biopython.org/DIST/docs/tutorial/Tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade numpy\n",
    "from __future__ import print_function\n",
    "search_term = 'type+1+diabetes[MH]'\n",
    "max_articles = 10000\n",
    "\n",
    "from Bio import Entrez\n",
    "print('Searching PubMed abstracts for documents containing term: ',search_term)\n",
    "handle = Entrez.esearch(db=\"pubmed\", term=search_term, retmax=max_articles)\n",
    "record = Entrez.read(handle)\n",
    "handle.close()\n",
    "idlist = record[\"IdList\"]\n",
    "\n",
    "print('Found:',len(idlist),' documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fetching the previously found documents\n",
    "#select only English articles \n",
    "#assign labels based on the keywords assoociated with the articles\n",
    "from Bio import Medline\n",
    "handle = Entrez.efetch(db=\"pubmed\", id=idlist, rettype=\"medline\",retmode=\"text\")\n",
    "records = Medline.parse(handle)\n",
    "data = []\n",
    "adults =0\n",
    "child =0\n",
    "for record in records:\n",
    "    if 'AB' not in record or record['AB'] is None:\n",
    "        continue\n",
    "    if len(record['LA'])==1 and record['LA'][0]=='eng':\n",
    "        is_adult = False\n",
    "        is_child = False\n",
    "        for val in record['MH']:\n",
    "            if val =='Adult':\n",
    "                is_adult=True\n",
    "                break\n",
    "            if val =='Adolescent' or val=='Child':\n",
    "                is_child=True\n",
    "        if is_adult and is_child:\n",
    "            continue\n",
    "        if is_adult:\n",
    "            adults+=1\n",
    "            data.append((record,1))\n",
    "        if is_child:\n",
    "            data.append((record,0))\n",
    "            child+=1\n",
    "print ('Articles about adults:',adults,'  Articles about children/child:',child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #Dump the obtained data to disk in case we need to repeat the process from here on.\n",
    "# import dill as pickle\n",
    "# with open('pubmed_records.tmp','w') as f:\n",
    "#     pickle.dump(data,f)\n",
    "# # pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Split the data into trianing/test sets\n",
    "split=0.8\n",
    "train_set = data[:int(split*len(data))]\n",
    "test_set = data[int(split*len(data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#separate labels \n",
    "x_train = [record[0].get('AB') for record in train_set]\n",
    "y_train = [record[1] for record in train_set]\n",
    "x_test = [record[0].get('AB') for record in test_set]\n",
    "y_test = [record[1] for record in test_set]\n",
    "\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "\n",
    "max_features = 5000\n",
    "#Transform the input articles into number sequences by replacing each word\n",
    "#with it's index in a frequency list\n",
    "\n",
    "x_train = [hashing_trick(record,max_features) for record in x_train]\n",
    "x_test = [hashing_trick(record,max_features) for record in x_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "# set parameters for our model:\n",
    "maxlen = 1000 #max 1000 words per article\n",
    "batch_size = 32 #size of the batch \n",
    "embedding_dims = 50 # size of the embedding vectors for each word\n",
    "filters = 250 #dimension of filters for the convolutional layer\n",
    "kernel_size = 3 #size of the kernel used in the convolutional layer\n",
    "hidden_dims = 250 #dimension of the hidden layer\n",
    "epochs = 5 #number of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen\n",
    "                   ))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy',f1])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy',f1])\n",
    "score = loaded_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Performance of loaded model on the test-set:')\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[2], score[2]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
