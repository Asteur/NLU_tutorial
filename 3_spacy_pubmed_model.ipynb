{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>How to build a document classifier ? </h4>\n",
    "Problem:\n",
    "\n",
    "We want to build a model that is able to tell if a Pubmed article is refering to child or adult patient(s).\n",
    "\n",
    "Solution:\n",
    "\n",
    "We construct a training/validation set out of English only Pubmed articles and use the keywords associated with these articles to assign the labels.\n",
    "\n",
    "Using Keras with Tensorflow backend, we train a convolutional neural network on the training data. We show very good accuracy and f1-score can be obtained in 5 Epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://biopython.org/DIST/docs/tutorial/Tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching PubMed abstracts for documents containing term:  type+1+diabetes[MH]\n",
      "Found: 10000  documents\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "search_term = 'type+1+diabetes[MH]'\n",
    "max_articles = 10000\n",
    "\n",
    "from Bio import Entrez\n",
    "print('Searching PubMed abstracts for documents containing term: ',search_term)\n",
    "handle = Entrez.esearch(db=\"pubmed\", term=search_term, retmax=max_articles)\n",
    "record = Entrez.read(handle)\n",
    "handle.close()\n",
    "idlist = record[\"IdList\"]\n",
    "\n",
    "print('Found:',len(idlist),' documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles about adults: 2005   Articles about children/child: 1580\n"
     ]
    }
   ],
   "source": [
    "#fetching the previously found documents\n",
    "#select only English articles \n",
    "#assign labels based on the keywords assoociated with the articles\n",
    "from Bio import Medline\n",
    "handle = Entrez.efetch(db=\"pubmed\", id=idlist, rettype=\"medline\",retmode=\"text\")\n",
    "records = Medline.parse(handle)\n",
    "data = []\n",
    "adults =0\n",
    "child =0\n",
    "for record in records:\n",
    "    if 'AB' not in record or record['AB'] is None:\n",
    "        continue\n",
    "    if len(record['LA'])==1 and record['LA'][0]=='eng':\n",
    "        is_adult = False\n",
    "        is_child = False\n",
    "        for val in record['MH']:\n",
    "            if val =='Adult':\n",
    "                is_adult=True\n",
    "                break\n",
    "            if val =='Adolescent' or val=='Child':\n",
    "                is_child=True\n",
    "        if is_adult and is_child:\n",
    "            continue\n",
    "        if is_adult:\n",
    "            adults+=1\n",
    "            data.append((record,1))\n",
    "        if is_child:\n",
    "            data.append((record,0))\n",
    "            child+=1\n",
    "print ('Articles about adults:',adults,'  Articles about children/child:',child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Dump the obtained data to disk in case we need to repeat the process from here on.\n",
    "import dill as pickle\n",
    "with open('pubmed_records.tmp','w') as f:\n",
    "    pickle.dump(data,f)\n",
    "# pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Split the data into trianing/test sets\n",
    "split=0.8\n",
    "train_set = data[:int(split*len(data))]\n",
    "test_set = data[int(split*len(data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#separate labels \n",
    "x_train = [record[0].get('AB') for record in train_set]\n",
    "y_train = [record[1] for record in train_set]\n",
    "x_test = [record[0].get('AB') for record in test_set]\n",
    "y_test = [record[1] for record in test_set]\n",
    "\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "\n",
    "max_features = 5000\n",
    "#Transform the input articles into number sequences by replacing each word\n",
    "#with it's index in a frequency list\n",
    "\n",
    "x_train = [hashing_trick(record,max_features) for record in x_train]\n",
    "x_test = [hashing_trick(record,max_features) for record in x_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "# set parameters for our model:\n",
    "maxlen = 1000 #max 1000 words per article\n",
    "batch_size = 32 #size of the batch \n",
    "embedding_dims = 50 # size of the embedding vectors for each word\n",
    "filters = 250 #dimension of filters for the convolutional layer\n",
    "kernel_size = 3 #size of the kernel used in the convolutional layer\n",
    "hidden_dims = 250 #dimension of the hidden layer\n",
    "epochs = 5 #number of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2868 train sequences\n",
      "717 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (2868, 1000)\n",
      "x_test shape: (717, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train on 2868 samples, validate on 717 samples\n",
      "Epoch 1/5\n",
      "2868/2868 [==============================] - 32s - loss: 0.6252 - acc: 0.6496 - f1: 0.7627 - val_loss: 0.3850 - val_acc: 0.8424 - val_f1: 0.8723\n",
      "Epoch 2/5\n",
      "2868/2868 [==============================] - 34s - loss: 0.2890 - acc: 0.8856 - f1: 0.9008 - val_loss: 0.2575 - val_acc: 0.8954 - val_f1: 0.9071\n",
      "Epoch 3/5\n",
      "2868/2868 [==============================] - 34s - loss: 0.1825 - acc: 0.9264 - f1: 0.9346 - val_loss: 0.2379 - val_acc: 0.9010 - val_f1: 0.9126\n",
      "Epoch 4/5\n",
      "2868/2868 [==============================] - 33s - loss: 0.0983 - acc: 0.9658 - f1: 0.9684 - val_loss: 0.2756 - val_acc: 0.9024 - val_f1: 0.9160\n",
      "Epoch 5/5\n",
      "2868/2868 [==============================] - 34s - loss: 0.0371 - acc: 0.9899 - f1: 0.9900 - val_loss: 0.2975 - val_acc: 0.8884 - val_f1: 0.9009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11c3ad6d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen\n",
    "                   ))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy',f1])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"702pt\" viewBox=\"0.00 0.00 299.71 702.00\" width=\"300pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 698)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-698 295.707,-698 295.707,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 4767027984 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>4767027984</title>\n",
       "<polygon fill=\"none\" points=\"47.0688,-657.5 47.0688,-693.5 244.6382,-693.5 244.6382,-657.5 47.0688,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"145.8535\" y=\"-671.3\">embedding_1_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 4767027856 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>4767027856</title>\n",
       "<polygon fill=\"none\" points=\"63.7827,-584.5 63.7827,-620.5 227.9243,-620.5 227.9243,-584.5 63.7827,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"145.8535\" y=\"-598.3\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 4767027984&#45;&gt;4767027856 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>4767027984-&gt;4767027856</title>\n",
       "<path d=\"M145.8535,-657.4551C145.8535,-649.3828 145.8535,-639.6764 145.8535,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"149.3536,-630.5903 145.8535,-620.5904 142.3536,-630.5904 149.3536,-630.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4767222800 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>4767222800</title>\n",
       "<polygon fill=\"none\" points=\"82.0518,-511.5 82.0518,-547.5 209.6553,-547.5 209.6553,-511.5 82.0518,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"145.8535\" y=\"-525.3\">dropout_1: Dropout</text>\n",
       "</g>\n",
       "<!-- 4767027856&#45;&gt;4767222800 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>4767027856-&gt;4767222800</title>\n",
       "<path d=\"M145.8535,-584.4551C145.8535,-576.3828 145.8535,-566.6764 145.8535,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"149.3536,-557.5903 145.8535,-547.5904 142.3536,-557.5904 149.3536,-557.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4767223440 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>4767223440</title>\n",
       "<polygon fill=\"none\" points=\"82.8276,-438.5 82.8276,-474.5 208.8794,-474.5 208.8794,-438.5 82.8276,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"145.8535\" y=\"-452.3\">conv1d_1: Conv1D</text>\n",
       "</g>\n",
       "<!-- 4767222800&#45;&gt;4767223440 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>4767222800-&gt;4767223440</title>\n",
       "<path d=\"M145.8535,-511.4551C145.8535,-503.3828 145.8535,-493.6764 145.8535,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"149.3536,-484.5903 145.8535,-474.5904 142.3536,-484.5904 149.3536,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4767699536 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>4767699536</title>\n",
       "<polygon fill=\"none\" points=\"0,-365.5 0,-401.5 291.707,-401.5 291.707,-365.5 0,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"145.8535\" y=\"-379.3\">global_max_pooling1d_1: GlobalMaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 4767223440&#45;&gt;4767699536 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>4767223440-&gt;4767699536</title>\n",
       "<path d=\"M145.8535,-438.4551C145.8535,-430.3828 145.8535,-420.6764 145.8535,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"149.3536,-411.5903 145.8535,-401.5904 142.3536,-411.5904 149.3536,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4767028624 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>4767028624</title>\n",
       "<polygon fill=\"none\" points=\"93.7275,-292.5 93.7275,-328.5 197.9795,-328.5 197.9795,-292.5 93.7275,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"145.8535\" y=\"-306.3\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 4767699536&#45;&gt;4767028624 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>4767699536-&gt;4767028624</title>\n",
       "<path d=\"M145.8535,-365.4551C145.8535,-357.3828 145.8535,-347.6764 145.8535,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"149.3536,-338.5903 145.8535,-328.5904 142.3536,-338.5904 149.3536,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4767750800 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>4767750800</title>\n",
       "<polygon fill=\"none\" points=\"82.0518,-219.5 82.0518,-255.5 209.6553,-255.5 209.6553,-219.5 82.0518,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"145.8535\" y=\"-233.3\">dropout_2: Dropout</text>\n",
       "</g>\n",
       "<!-- 4767028624&#45;&gt;4767750800 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>4767028624-&gt;4767750800</title>\n",
       "<path d=\"M145.8535,-292.4551C145.8535,-284.3828 145.8535,-274.6764 145.8535,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"149.3536,-265.5903 145.8535,-255.5904 142.3536,-265.5904 149.3536,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4767751312 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>4767751312</title>\n",
       "<polygon fill=\"none\" points=\"70.3931,-146.5 70.3931,-182.5 221.314,-182.5 221.314,-146.5 70.3931,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"145.8535\" y=\"-160.3\">activation_1: Activation</text>\n",
       "</g>\n",
       "<!-- 4767750800&#45;&gt;4767751312 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>4767750800-&gt;4767751312</title>\n",
       "<path d=\"M145.8535,-219.4551C145.8535,-211.3828 145.8535,-201.6764 145.8535,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"149.3536,-192.5903 145.8535,-182.5904 142.3536,-192.5904 149.3536,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4768006352 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>4768006352</title>\n",
       "<polygon fill=\"none\" points=\"93.7275,-73.5 93.7275,-109.5 197.9795,-109.5 197.9795,-73.5 93.7275,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"145.8535\" y=\"-87.3\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 4767751312&#45;&gt;4768006352 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>4767751312-&gt;4768006352</title>\n",
       "<path d=\"M145.8535,-146.4551C145.8535,-138.3828 145.8535,-128.6764 145.8535,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"149.3536,-119.5903 145.8535,-109.5904 142.3536,-119.5904 149.3536,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4767874640 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>4767874640</title>\n",
       "<polygon fill=\"none\" points=\"70.3931,-.5 70.3931,-36.5 221.314,-36.5 221.314,-.5 70.3931,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"145.8535\" y=\"-14.3\">activation_2: Activation</text>\n",
       "</g>\n",
       "<!-- 4768006352&#45;&gt;4767874640 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>4768006352-&gt;4767874640</title>\n",
       "<path d=\"M145.8535,-73.4551C145.8535,-65.3828 145.8535,-55.6764 145.8535,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"149.3536,-46.5903 145.8535,-36.5904 142.3536,-46.5904 149.3536,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of loaded model on the test-set:\n",
      "acc: 88.84%\n",
      "f1: 90.09%\n"
     ]
    }
   ],
   "source": [
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy',f1])\n",
    "score = loaded_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Performance of loaded model on the test-set:')\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[2], score[2]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
