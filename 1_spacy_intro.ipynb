{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>This is a very short introduction to SpaCy.</H1>\n",
    "for further reading please navigate to:\n",
    "https://alpha.spacy.io/docs/usage/spacy-101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>What is spaCy? </h4>\n",
    "<p>\n",
    "<ul>\n",
    "<li>spaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python.</li>\n",
    "<li>\n",
    "spaCy is designed specifically for <b><em>production use</em></b> and helps you build applications that process and \"understand\" large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning.</li>\n",
    "</ul></p>\n",
    "<h4>What does it do?</h4>\n",
    "<ul>\n",
    "<li>Tokenization<p>   Segmenting text into words, punctuations marks etc.</p></li>\n",
    "<li>Part-of-speech (POS) Tagging <p>Assigning word types to tokens, like verb or noun.</p>\t</li>\n",
    "<li>Dependency Parsing<p>\tAssigning syntactic dependency labels, describing the relations between individual tokens, like subject or object.</p>\t</li>\n",
    "<li>Lemmatization<p>\tAssigning the base forms of words. For example, the lemma of \"was\" is \"be\", and the lemma of \"rats\" is \"rat\".</p>\t</li>\n",
    "<li>Sentence Boundary Detection (SBD)<p>\tFinding and segmenting individual sentences.</p>\t</li>\n",
    "<li>Named Entity Recongition (NER)\t<p>Labelling named \"real-world\" objects, like persons, companies or locations.</p>\t</li>\n",
    "<li>Similarity<p>\tComparing words, text spans and documents and how similar they are to each other.</p>\t</li>\n",
    "<li>Text classification\t<p>Assigning categories or labels to a whole document, or parts of a document.\t</p></li>\n",
    "<li>Rule-based Matching\t<p>Finding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions.</p>\t</li>\n",
    "<li>Model Training\t<p>Updating and improving all statistical models.<p>\t</li>\n",
    "<li>Language Data<p> Comes packed with laguage models (and data) for various languages (English,German,Spanish,French)</p></li>\n",
    "</ul>\n",
    "<h4>Architecture Overview</h4>\n",
    "<img src='architecture.svg'></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let's see it in action</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: en\n",
      "Vocabulary size: 742225\n",
      "Default NLP Pipeline:\n",
      "\t <spacy.tagger.Tagger object at 0x10f878050>\n",
      "\t <spacy.pipeline.DependencyParser object at 0x107a183c0>\n",
      "\t <spacy.matcher.Matcher object at 0x107912578>\n",
      "\t <spacy.pipeline.EntityRecognizer object at 0x107a189f0>\n"
     ]
    }
   ],
   "source": [
    "#Load the English language model in SpaCy\n",
    "# this takes a little longer because there's a lot of data to load\n",
    "import spacy                        \n",
    "nlp = spacy.load('en')\n",
    "print('Language:',nlp.lang)\n",
    "print('Vocabulary size:',nlp.vocab.length)\n",
    "print(\"Default NLP Pipeline:\")\n",
    "for obj in nlp.pipeline:\n",
    "    print('\\t',obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Default Pipeline</h4>\n",
    "<img src = 'pipeline.svg'></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prescribing sick days due to diagnosis of influenza.Jane complains about flu-like symptoms.Jane may be experiencing some sort of flu episode.Jane’s RIDT came back negative for influenza.\n",
      "Jane is at high risk for flu if she’s not vaccinated.Jane’s older brother had the flu last month.Jane had a severe case of flu last year.Joe expressed concerns about the risks of bird flu.\n",
      "Joe shows no signs of stroke, except for numbness.Nausea, vomiting and ankle swelling negative.Patient denies alcohol abuse. Allergies: Penicillin, Dust, Sneezing.\n",
      "There's an outbreak of happiness in New York organized by O'Reilly Media, today, September 26, 2017, involving thousands of people.\n"
     ]
    }
   ],
   "source": [
    "#Input text\n",
    "txt =u\"\"\"Prescribing sick days due to diagnosis of influenza.Jane complains about flu-like symptoms.Jane may be experiencing some sort of flu episode.Jane’s RIDT came back negative for influenza.\n",
    "Jane is at high risk for flu if she’s not vaccinated.Jane’s older brother had the flu last month.Jane had a severe case of flu last year.Joe expressed concerns about the risks of bird flu.\n",
    "Joe shows no signs of stroke, except for numbness.Nausea, vomiting and ankle swelling negative.Patient denies alcohol abuse. Allergies: Penicillin, Dust, Sneezing.\n",
    "There's an outbreak of happiness in New York organized by O'Reilly Media, today, September 26, 2017, involving thousands of people.\"\"\"\n",
    "\n",
    "#Call Spacy on the input text\n",
    "#This runs the standard NLP pipeline on the input text\n",
    "doc = nlp(txt) \n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Sentence detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>Sentence Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>Prescribing sick days due to diagnosis of infl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>Jane complains about flu-like symptoms.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>27</td>\n",
       "      <td>Jane may be experiencing some sort of flu epis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>37</td>\n",
       "      <td>Jane’s RIDT came back negative for influenza.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>50</td>\n",
       "      <td>Jane is at high risk for flu if she’s not vacc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>60</td>\n",
       "      <td>Jane’s older brother had the flu last month.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>60</td>\n",
       "      <td>70</td>\n",
       "      <td>Jane had a severe case of flu last year.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>70</td>\n",
       "      <td>81</td>\n",
       "      <td>Joe expressed concerns about the risks of bird...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>81</td>\n",
       "      <td>92</td>\n",
       "      <td>Joe shows no signs of stroke, except for numbn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>92</td>\n",
       "      <td>100</td>\n",
       "      <td>Nausea, vomiting and ankle swelling negative.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100</td>\n",
       "      <td>105</td>\n",
       "      <td>Patient denies alcohol abuse.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>105</td>\n",
       "      <td>114</td>\n",
       "      <td>Allergies: Penicillin, Dust, Sneezing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>114</td>\n",
       "      <td>140</td>\n",
       "      <td>There's an outbreak of happiness in New York o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Start  End                                      Sentence Text\n",
       "0       0    9  Prescribing sick days due to diagnosis of infl...\n",
       "1       9   17            Jane complains about flu-like symptoms.\n",
       "2      17   27  Jane may be experiencing some sort of flu epis...\n",
       "3      27   37      Jane’s RIDT came back negative for influenza.\n",
       "4      37   50  Jane is at high risk for flu if she’s not vacc...\n",
       "5      50   60       Jane’s older brother had the flu last month.\n",
       "6      60   70           Jane had a severe case of flu last year.\n",
       "7      70   81  Joe expressed concerns about the risks of bird...\n",
       "8      81   92  Joe shows no signs of stroke, except for numbn...\n",
       "9      92  100      Nausea, vomiting and ankle swelling negative.\n",
       "10    100  105                      Patient denies alcohol abuse.\n",
       "11    105  114             Allergies: Penicillin, Dust, Sneezing.\n",
       "12    114  140  There's an outbreak of happiness in New York o..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=[]\n",
    "for sent in doc.sents:\n",
    "    data.append((sent.start,sent.end,sent.text.replace('\\n','')))\n",
    "#For display purposes only we put the sentence boundry information in a Pandas DataFrame\n",
    "import pandas as pd\n",
    "sents = pd.DataFrame(data=data,columns = ['Start','End','Sentence Text'])\n",
    "sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech tagging and Named Entity extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Token</th>\n",
       "      <th>Id_in_vocab</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>Depends_on</th>\n",
       "      <th>Dependency_type</th>\n",
       "      <th>Entity_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Prescribing</td>\n",
       "      <td>102853</td>\n",
       "      <td>prescribe</td>\n",
       "      <td>VERB</td>\n",
       "      <td>Prescribing</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>sick</td>\n",
       "      <td>1057</td>\n",
       "      <td>sick</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>days</td>\n",
       "      <td>amod</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>days</td>\n",
       "      <td>420</td>\n",
       "      <td>day</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>Prescribing</td>\n",
       "      <td>dobj</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>due</td>\n",
       "      <td>688</td>\n",
       "      <td>due</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>days</td>\n",
       "      <td>amod</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>to</td>\n",
       "      <td>4</td>\n",
       "      <td>to</td>\n",
       "      <td>ADP</td>\n",
       "      <td>due</td>\n",
       "      <td>prep</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>29</td>\n",
       "      <td>diagnosis</td>\n",
       "      <td>403187</td>\n",
       "      <td>diagnosi</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>to</td>\n",
       "      <td>pobj</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>39</td>\n",
       "      <td>of</td>\n",
       "      <td>7</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>diagnosis</td>\n",
       "      <td>prep</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>42</td>\n",
       "      <td>influenza</td>\n",
       "      <td>66251</td>\n",
       "      <td>influenza</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>of</td>\n",
       "      <td>pobj</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>51</td>\n",
       "      <td>.</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>Prescribing</td>\n",
       "      <td>punct</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>52</td>\n",
       "      <td>Jane</td>\n",
       "      <td>355245</td>\n",
       "      <td>jane</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>complains</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>57</td>\n",
       "      <td>complains</td>\n",
       "      <td>527780</td>\n",
       "      <td>complain</td>\n",
       "      <td>VERB</td>\n",
       "      <td>complains</td>\n",
       "      <td>ROOT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>67</td>\n",
       "      <td>about</td>\n",
       "      <td>49</td>\n",
       "      <td>about</td>\n",
       "      <td>ADP</td>\n",
       "      <td>complains</td>\n",
       "      <td>prep</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>73</td>\n",
       "      <td>flu</td>\n",
       "      <td>428081</td>\n",
       "      <td>flu</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>like</td>\n",
       "      <td>npadvmod</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>76</td>\n",
       "      <td>-</td>\n",
       "      <td>22</td>\n",
       "      <td>-</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>like</td>\n",
       "      <td>punct</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>77</td>\n",
       "      <td>like</td>\n",
       "      <td>45</td>\n",
       "      <td>like</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>symptoms</td>\n",
       "      <td>amod</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>82</td>\n",
       "      <td>symptoms</td>\n",
       "      <td>7185</td>\n",
       "      <td>symptom</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>about</td>\n",
       "      <td>pobj</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>90</td>\n",
       "      <td>.</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>complains</td>\n",
       "      <td>punct</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>91</td>\n",
       "      <td>Jane</td>\n",
       "      <td>355245</td>\n",
       "      <td>jane</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>experiencing</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>96</td>\n",
       "      <td>may</td>\n",
       "      <td>213</td>\n",
       "      <td>may</td>\n",
       "      <td>VERB</td>\n",
       "      <td>experiencing</td>\n",
       "      <td>aux</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>100</td>\n",
       "      <td>be</td>\n",
       "      <td>23</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>experiencing</td>\n",
       "      <td>aux</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>103</td>\n",
       "      <td>experiencing</td>\n",
       "      <td>7290</td>\n",
       "      <td>experience</td>\n",
       "      <td>VERB</td>\n",
       "      <td>experiencing</td>\n",
       "      <td>ROOT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>116</td>\n",
       "      <td>some</td>\n",
       "      <td>86</td>\n",
       "      <td>some</td>\n",
       "      <td>DET</td>\n",
       "      <td>sort</td>\n",
       "      <td>det</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>121</td>\n",
       "      <td>sort</td>\n",
       "      <td>481</td>\n",
       "      <td>sort</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>experiencing</td>\n",
       "      <td>dobj</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>126</td>\n",
       "      <td>of</td>\n",
       "      <td>7</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>sort</td>\n",
       "      <td>prep</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>129</td>\n",
       "      <td>flu</td>\n",
       "      <td>428081</td>\n",
       "      <td>flu</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>episode</td>\n",
       "      <td>compound</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>133</td>\n",
       "      <td>episode</td>\n",
       "      <td>2318</td>\n",
       "      <td>episode</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>of</td>\n",
       "      <td>pobj</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>140</td>\n",
       "      <td>.</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>experiencing</td>\n",
       "      <td>punct</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>141</td>\n",
       "      <td>Jane</td>\n",
       "      <td>355245</td>\n",
       "      <td>jane</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>RIDT</td>\n",
       "      <td>compound</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>145</td>\n",
       "      <td>’s</td>\n",
       "      <td>619</td>\n",
       "      <td>’s</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>RIDT</td>\n",
       "      <td>compound</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>148</td>\n",
       "      <td>RIDT</td>\n",
       "      <td>0</td>\n",
       "      <td>ridt</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>came</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>528</td>\n",
       "      <td>,</td>\n",
       "      <td>2</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>Dust</td>\n",
       "      <td>punct</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>530</td>\n",
       "      <td>Sneezing</td>\n",
       "      <td>647645</td>\n",
       "      <td>sneezing</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>Allergies</td>\n",
       "      <td>appos</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>538</td>\n",
       "      <td>.</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>Allergies</td>\n",
       "      <td>punct</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>539</td>\n",
       "      <td>\\n</td>\n",
       "      <td>72</td>\n",
       "      <td>\\n</td>\n",
       "      <td>SPACE</td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>540</td>\n",
       "      <td>There</td>\n",
       "      <td>172</td>\n",
       "      <td>there</td>\n",
       "      <td>ADV</td>\n",
       "      <td>'s</td>\n",
       "      <td>expl</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>545</td>\n",
       "      <td>'s</td>\n",
       "      <td>17</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>'s</td>\n",
       "      <td>ROOT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>548</td>\n",
       "      <td>an</td>\n",
       "      <td>56</td>\n",
       "      <td>an</td>\n",
       "      <td>DET</td>\n",
       "      <td>outbreak</td>\n",
       "      <td>det</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>551</td>\n",
       "      <td>outbreak</td>\n",
       "      <td>443618</td>\n",
       "      <td>outbreak</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>'s</td>\n",
       "      <td>attr</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>560</td>\n",
       "      <td>of</td>\n",
       "      <td>7</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>outbreak</td>\n",
       "      <td>prep</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>563</td>\n",
       "      <td>happiness</td>\n",
       "      <td>4061</td>\n",
       "      <td>happines</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>of</td>\n",
       "      <td>pobj</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>573</td>\n",
       "      <td>in</td>\n",
       "      <td>14</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>happiness</td>\n",
       "      <td>prep</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>576</td>\n",
       "      <td>New</td>\n",
       "      <td>740</td>\n",
       "      <td>new</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>York</td>\n",
       "      <td>compound</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>580</td>\n",
       "      <td>York</td>\n",
       "      <td>2149</td>\n",
       "      <td>york</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>in</td>\n",
       "      <td>pobj</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>585</td>\n",
       "      <td>organized</td>\n",
       "      <td>3289</td>\n",
       "      <td>organize</td>\n",
       "      <td>VERB</td>\n",
       "      <td>happiness</td>\n",
       "      <td>acl</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>595</td>\n",
       "      <td>by</td>\n",
       "      <td>66</td>\n",
       "      <td>by</td>\n",
       "      <td>ADP</td>\n",
       "      <td>organized</td>\n",
       "      <td>agent</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>598</td>\n",
       "      <td>O'Reilly</td>\n",
       "      <td>3515</td>\n",
       "      <td>o'reilly</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>Media</td>\n",
       "      <td>compound</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>607</td>\n",
       "      <td>Media</td>\n",
       "      <td>6035</td>\n",
       "      <td>media</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>by</td>\n",
       "      <td>pobj</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>612</td>\n",
       "      <td>,</td>\n",
       "      <td>2</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>Media</td>\n",
       "      <td>punct</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>614</td>\n",
       "      <td>today</td>\n",
       "      <td>461</td>\n",
       "      <td>today</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>Media</td>\n",
       "      <td>npadvmod</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>619</td>\n",
       "      <td>,</td>\n",
       "      <td>2</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>today</td>\n",
       "      <td>punct</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>621</td>\n",
       "      <td>September</td>\n",
       "      <td>4268</td>\n",
       "      <td>september</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>today</td>\n",
       "      <td>npadvmod</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>631</td>\n",
       "      <td>26</td>\n",
       "      <td>5644</td>\n",
       "      <td>26</td>\n",
       "      <td>NUM</td>\n",
       "      <td>September</td>\n",
       "      <td>nummod</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>633</td>\n",
       "      <td>,</td>\n",
       "      <td>2</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>September</td>\n",
       "      <td>punct</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>635</td>\n",
       "      <td>2017</td>\n",
       "      <td>626680</td>\n",
       "      <td>2017</td>\n",
       "      <td>NUM</td>\n",
       "      <td>September</td>\n",
       "      <td>appos</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>639</td>\n",
       "      <td>,</td>\n",
       "      <td>2</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>September</td>\n",
       "      <td>punct</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>641</td>\n",
       "      <td>involving</td>\n",
       "      <td>4705</td>\n",
       "      <td>involve</td>\n",
       "      <td>VERB</td>\n",
       "      <td>September</td>\n",
       "      <td>acl</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>651</td>\n",
       "      <td>thousands</td>\n",
       "      <td>1347</td>\n",
       "      <td>thousand</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>involving</td>\n",
       "      <td>dobj</td>\n",
       "      <td>CARDINAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>661</td>\n",
       "      <td>of</td>\n",
       "      <td>7</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>thousands</td>\n",
       "      <td>prep</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>664</td>\n",
       "      <td>people</td>\n",
       "      <td>47</td>\n",
       "      <td>people</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>of</td>\n",
       "      <td>pobj</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>670</td>\n",
       "      <td>.</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>'s</td>\n",
       "      <td>punct</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Index         Token  Id_in_vocab       Lemma    POS    Depends_on  \\\n",
       "0        0   Prescribing       102853   prescribe   VERB   Prescribing   \n",
       "1       12          sick         1057        sick    ADJ          days   \n",
       "2       17          days          420         day   NOUN   Prescribing   \n",
       "3       22           due          688         due    ADJ          days   \n",
       "4       26            to            4          to    ADP           due   \n",
       "5       29     diagnosis       403187    diagnosi   NOUN            to   \n",
       "6       39            of            7          of    ADP     diagnosis   \n",
       "7       42     influenza        66251   influenza   NOUN            of   \n",
       "8       51             .            1           .  PUNCT   Prescribing   \n",
       "9       52          Jane       355245        jane  PROPN     complains   \n",
       "10      57     complains       527780    complain   VERB     complains   \n",
       "11      67         about           49       about    ADP     complains   \n",
       "12      73           flu       428081         flu   NOUN          like   \n",
       "13      76             -           22           -  PUNCT          like   \n",
       "14      77          like           45        like    ADJ      symptoms   \n",
       "15      82      symptoms         7185     symptom   NOUN         about   \n",
       "16      90             .            1           .  PUNCT     complains   \n",
       "17      91          Jane       355245        jane  PROPN  experiencing   \n",
       "18      96           may          213         may   VERB  experiencing   \n",
       "19     100            be           23          be   VERB  experiencing   \n",
       "20     103  experiencing         7290  experience   VERB  experiencing   \n",
       "21     116          some           86        some    DET          sort   \n",
       "22     121          sort          481        sort   NOUN  experiencing   \n",
       "23     126            of            7          of    ADP          sort   \n",
       "24     129           flu       428081         flu   NOUN       episode   \n",
       "25     133       episode         2318     episode   NOUN            of   \n",
       "26     140             .            1           .  PUNCT  experiencing   \n",
       "27     141          Jane       355245        jane  PROPN          RIDT   \n",
       "28     145            ’s          619          ’s  PROPN          RIDT   \n",
       "29     148          RIDT            0        ridt  PROPN          came   \n",
       "..     ...           ...          ...         ...    ...           ...   \n",
       "110    528             ,            2           ,  PUNCT          Dust   \n",
       "111    530      Sneezing       647645    sneezing   NOUN     Allergies   \n",
       "112    538             .            1           .  PUNCT     Allergies   \n",
       "113    539            \\n           72          \\n  SPACE             .   \n",
       "114    540         There          172       there    ADV            's   \n",
       "115    545            's           17          be   VERB            's   \n",
       "116    548            an           56          an    DET      outbreak   \n",
       "117    551      outbreak       443618    outbreak   NOUN            's   \n",
       "118    560            of            7          of    ADP      outbreak   \n",
       "119    563     happiness         4061    happines   NOUN            of   \n",
       "120    573            in           14          in    ADP     happiness   \n",
       "121    576           New          740         new  PROPN          York   \n",
       "122    580          York         2149        york  PROPN            in   \n",
       "123    585     organized         3289    organize   VERB     happiness   \n",
       "124    595            by           66          by    ADP     organized   \n",
       "125    598      O'Reilly         3515    o'reilly  PROPN         Media   \n",
       "126    607         Media         6035       media  PROPN            by   \n",
       "127    612             ,            2           ,  PUNCT         Media   \n",
       "128    614         today          461       today   NOUN         Media   \n",
       "129    619             ,            2           ,  PUNCT         today   \n",
       "130    621     September         4268   september  PROPN         today   \n",
       "131    631            26         5644          26    NUM     September   \n",
       "132    633             ,            2           ,  PUNCT     September   \n",
       "133    635          2017       626680        2017    NUM     September   \n",
       "134    639             ,            2           ,  PUNCT     September   \n",
       "135    641     involving         4705     involve   VERB     September   \n",
       "136    651     thousands         1347    thousand   NOUN     involving   \n",
       "137    661            of            7          of    ADP     thousands   \n",
       "138    664        people           47      people   NOUN            of   \n",
       "139    670             .            1           .  PUNCT            's   \n",
       "\n",
       "    Dependency_type Entity_Type  \n",
       "0              ROOT        DATE  \n",
       "1              amod        DATE  \n",
       "2              dobj        DATE  \n",
       "3              amod              \n",
       "4              prep              \n",
       "5              pobj              \n",
       "6              prep              \n",
       "7              pobj              \n",
       "8             punct              \n",
       "9             nsubj      PERSON  \n",
       "10             ROOT              \n",
       "11             prep              \n",
       "12         npadvmod              \n",
       "13            punct              \n",
       "14             amod              \n",
       "15             pobj              \n",
       "16            punct              \n",
       "17            nsubj      PERSON  \n",
       "18              aux              \n",
       "19              aux              \n",
       "20             ROOT              \n",
       "21              det              \n",
       "22             dobj              \n",
       "23             prep              \n",
       "24         compound              \n",
       "25             pobj              \n",
       "26            punct              \n",
       "27         compound      PERSON  \n",
       "28         compound      PERSON  \n",
       "29            nsubj      PERSON  \n",
       "..              ...         ...  \n",
       "110           punct              \n",
       "111           appos              \n",
       "112           punct              \n",
       "113                              \n",
       "114            expl              \n",
       "115            ROOT              \n",
       "116             det              \n",
       "117            attr              \n",
       "118            prep              \n",
       "119            pobj              \n",
       "120            prep              \n",
       "121        compound         GPE  \n",
       "122            pobj         GPE  \n",
       "123             acl              \n",
       "124           agent              \n",
       "125        compound         ORG  \n",
       "126            pobj         ORG  \n",
       "127           punct              \n",
       "128        npadvmod              \n",
       "129           punct              \n",
       "130        npadvmod        DATE  \n",
       "131          nummod        DATE  \n",
       "132           punct        DATE  \n",
       "133           appos        DATE  \n",
       "134           punct              \n",
       "135             acl              \n",
       "136            dobj    CARDINAL  \n",
       "137            prep              \n",
       "138            pobj              \n",
       "139           punct              \n",
       "\n",
       "[140 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for sent in doc.sents:\n",
    "    for w in sent:\n",
    "        tmp=[]\n",
    "        tmp.append(w.idx)\n",
    "        tmp.append(w.text)\n",
    "        tmp.append(w.lex_id)\n",
    "        tmp.append(w.lemma_)\n",
    "        tmp.append(w.pos_)\n",
    "        tmp.append(w.head)\n",
    "        tmp.append(w.dep_)\n",
    "        tmp.append(w.ent_type_)\n",
    "        data.append(tmp)\n",
    "tokens = pd.DataFrame(data=data, columns = ['Index','Token','Id_in_vocab',\n",
    "        'Lemma','POS','Depends_on','Dependency_type','Entity_Type'])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the syntactic dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject:  Jane  verb:  complains\n",
      "Subject:  Jane  verb:  experiencing\n",
      "Subject:  RIDT  verb:  came\n",
      "Subject:  Jane  verb:  is\n",
      "Subject:  she  verb:  ’s\n",
      "Subject:  brother  verb:  had\n",
      "Subject:  Jane  verb:  had\n",
      "Subject:  Joe  verb:  expressed\n",
      "Subject:  Joe  verb:  shows\n",
      "Subject:  Patient  verb:  denies\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import nsubj, VERB\n",
    "# Finding a verb with a subject \n",
    "pairs = []\n",
    "for possible_subject in doc:\n",
    "    if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\n",
    "        pairs.append((possible_subject,possible_subject.head))\n",
    "\n",
    "for pair in pairs:\n",
    "    print('Subject: ',pair[0],' verb: ',pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costum pipeline... Adding negation detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loading NegEx and it's rules\n",
    "from negex import *\n",
    "rfile = open(r'negex_triggers.txt')\n",
    "irules = sortRules(rfile.readlines())\n",
    "\n",
    "#Define a new pipeline component (based on NegEx)\n",
    "#Every pipeline component gets a Doc object and needs to return one\n",
    "#To store custom data, spaCy currently has a document level variable: doc.user_data\n",
    "#To store our negated words we add the index of the word (i) to a set under the 'negated' key\n",
    "\n",
    "def negation_tag(doc):\n",
    "    doc.user_data['negated']=set()\n",
    "    for sent in doc.sents:\n",
    "        ph= set()\n",
    "        for word in sent:\n",
    "            if word.pos_!='ADP' and word.pos_!='PUNCT':\n",
    "                ph.add(word.text)\n",
    "        tagger = negTagger(sentence = sent.text, phrases = list(ph),rules = irules, negP=False)\n",
    "        scopes=  tagger.getScopes()\n",
    "        res = set()\n",
    "        for scope in scopes:\n",
    "            s = scope.replace('[NEGATED]','').replace('.','').replace(',','')\n",
    "            if ' ' in s:\n",
    "                for wd in s.split(' '):\n",
    "                    res.add(wd)\n",
    "            else:\n",
    "                res.add(s)\n",
    "                \n",
    "        for word in sent:\n",
    "            if word.text in res:\n",
    "                doc.user_data['negated'].add(word.i)\n",
    "    return doc\n",
    "\n",
    "#define a new pipleline including the negation_tag component\n",
    "def custom_pipeline(nlp):\n",
    "    return (nlp.tagger,nlp.parser,negation_tag)\n",
    "\n",
    "#need to re-initlaize spaCy with the new pipeline\n",
    "nlp_neg = spacy.load('en', create_pipeline=custom_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jane is at high risk for flu if she’s not vaccinated.\n",
      "Negated words:  [vaccinated]\n",
      "\n",
      "Joe shows no signs of stroke, except for numbness.\n",
      "Negated words:  [stroke]\n",
      "\n",
      "Patient denies alcohol abuse.\n",
      "Negated words:  [alcohol, abuse]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc2 = nlp_neg(txt)\n",
    "for sent in doc2.sents:\n",
    "    negs = []\n",
    "    for word in sent:\n",
    "        if word.i in doc2.user_data['negated']:\n",
    "            negs.append(word)\n",
    "    if len(negs)>0:\n",
    "        print(sent)\n",
    "        print('Negated words: ',negs)\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
